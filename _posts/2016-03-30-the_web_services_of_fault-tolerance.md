---
layout: post
title: 亿级 Web 系统的容错性建设
category: 技术
---

### 参考资料
1. [摆脱救火员，让系统自身具备“容错”能力-腾讯徐汉彬](http://mp.weixin.qq.com/s?__biz=MzI4NzE1NTYyMg==&mid=402483623&idx=1&sn=43ba706e3919bec0661fe338bdeba3da)

容错其实是系统健壮性的重要指标之一，为了保证服务的100%或者尽可能的100%的可用性，需要让系统本身具备“容错”能力。

### 程序层面
1. 重试机制：我们请求一个服务，如果服务请求失败，则重试一次。甚至重试前主动计算服务成功率，成功率过低，就直接不做重试行为，避免带来过高的流量冲击。<br />![](/static/image/20160330_1.png)
2. 合理的超时时间：因为超时时间如果设的过长，一旦较大概率出现超时异常，系统的吞吐率就会大面积下降，有可能耗尽所有的worker（资源被占据，全部在等待状态，x秒超时才释放）
3. 服务降级，自动屏蔽非核心分支异常。例如日志上报，这个环节出问题可以忽略，避免进程堵塞。

### 运维层面
1. 动态剔除或者恢复异常机器：所有后端服务或者存储，首先是部署为无状态的方式提供服务（一个服务通常很多台机器），然后加到一个公共的智能路由服务L5中。发现某个服务的机器异常的时候（成功率低于50%），就会自动剔除该机器，后续，会发出试探性的请求，确认等它恢复正常之后，再重新加回到服务机器组。<br />![](/static/image/20160330_3.png)
2. 服务解耦、物理隔离：服务的设计，要尽可能小和分离部署，如此，服务之间的耦合会比较小，一旦某个模块出问题，受到影响的模块就比较少，容错能力就会更强。(1)服务分离，大服务变成多个小服务。(2)轻重分离，物理隔离。物理隔离例如：服务器都是分布在ABC三个机房，假设B机房整个网络故障了，反向代理服务会将无法接受服务的B机房机器剔除，然后，剩下AC机房的服务器仍然可以正常为外界提供服务。<br />![](/static/image/20160330_4.png)

### 业务层面：避免“人的失误”
例如，活动上线之前，我们要求负责活动的同事需要验证一下“礼包领取逻辑”，也就是真实的去领取一次礼包。然而，这只是一个“口头约定”，实际上并不具备强制执行力，如果这位同事因为活动的礼包过多，而漏过其中一个礼包的验证流程（这种事情也的确偶尔会发生），上线后恰好这个礼包的领取出了问题，即“现网事故”。这个可以通过程序去保证的，确保这位同事的id的确领取过全部的礼包。做法其实挺简单的，就是让负责活动的同事设置一个验证活动的id，然后，程序在发货活动时，程序会自动检查每一个子活动项目中，是否有这个id的活动参与记录。如果都有参与记录，则说明这位同事完整地领取了全部礼包。
![](/static/image/20160330_2.png)
